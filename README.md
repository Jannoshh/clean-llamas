Clean and comprehensible implementation of the Llama architecture.

Built on top of the [ARENA 2.0 transformer implementation](https://github.com/callummcdougall/ARENA_2.0/tree/main/chapter1_transformers/exercises/part1_transformer_from_scratch)
### TODO:
- Rotary positional embeddings
- Test that the implementation of the grouped multi query attention is correct